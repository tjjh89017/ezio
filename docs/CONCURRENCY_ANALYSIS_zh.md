# EZIO ä¸¦ç™¼æ€§èˆ‡åŸ·è¡Œç·’å®‰å…¨æ€§åˆ†æ

**ç‰ˆæœ¬ï¼š** 1.0
**æ—¥æœŸï¼š** 2025-12-14
**ç„¦é»ï¼š** å¤šåŸ·è¡Œç·’ç£ç¢Ÿ I/O æ“ä½œèˆ‡åŸ·è¡Œç·’å®‰å…¨æ€§

---

## ç›®éŒ„
1. [libtorrent ä¸¦ç™¼æ¨¡å‹](#libtorrent-ä¸¦ç™¼æ¨¡å‹)
2. [ç•¶å‰åŸ·è¡Œç·’å®‰å…¨æ€§å•é¡Œ](#ç•¶å‰åŸ·è¡Œç·’å®‰å…¨æ€§å•é¡Œ)
3. [Boost åŸ·è¡Œç·’æ± å®¢è£½åŒ–](#boost-åŸ·è¡Œç·’æ± å®¢è£½åŒ–)
4. [åŸ·è¡Œç·’å®‰å…¨å¿«å–è¨­è¨ˆ](#åŸ·è¡Œç·’å®‰å…¨å¿«å–è¨­è¨ˆ)
5. [å¯¦ä½œå»ºè­°](#å¯¦ä½œå»ºè­°)

---

## libtorrent ä¸¦ç™¼æ¨¡å‹

### å®˜æ–¹è¦æ ¼

æ ¹æ“š libtorrent æ–‡ä»¶èˆ‡ç¨‹å¼ç¢¼åˆ†æï¼š

**é‡é»ï¼š**
1. **å¤šå€‹åŸ·è¡Œç·’å¯ä»¥åŒæ™‚å‘¼å« disk_interface**
2. **ç„¡æ³•ä¿è­‰å“ªå€‹åŸ·è¡Œç·’æœƒå‘¼å«å“ªå€‹æ–¹æ³•**
3. **disk_interface å¯¦ä½œå¿…é ˆæ˜¯åŸ·è¡Œç·’å®‰å…¨çš„**
4. **å›èª¿å‡½æ•¸æœƒè¢«ç™¼ä½ˆåˆ° io_contextï¼ˆå–®åŸ·è¡Œç·’ï¼‰**

### ä¾†è‡ªç¨‹å¼ç¢¼è¨»è§£çš„è­‰æ“š

**raw_disk_io.hpp:219-221:**
```cpp
// this is called when the session is starting to shut down. The disk
// I/O object is expected to flush any outstanding write jobs, cancel
// hash jobs and initiate tearing down of any internal threads. If
// ``wait`` is true, this should be asynchronous. i.e. this call should
// not return until all threads have stopped and all jobs have either
// been aborted or completed and the disk I/O object is ready to be
// destructed.
```

**raw_disk_io.hpp:226-229:**
```cpp
// This will be called after a batch of disk jobs has been issues (via
// the ``async_*`` ). It gives the disk I/O object an opportunity to
// notify any potential condition variables to wake up the disk
// thread(s). The ``async_*`` calls can of course also notify condition
// variables, but doing it in this call allows for batching jobs, by
// issuing the notification once for a collection of jobs.
```

### å‘¼å«æ¨¡å¼åˆ†æ

**å ´æ™¯ 1ï¼šå¤šå€‹å°ç­‰ç¯€é»ï¼Œå–®ä¸€åˆ†ç‰‡**
```
åŸ·è¡Œç·’ Aï¼ˆå°ç­‰ç¯€é» 1ï¼‰ï¼šasync_write(piece=5, offset=0)
åŸ·è¡Œç·’ Bï¼ˆå°ç­‰ç¯€é» 2ï¼‰ï¼šasync_write(piece=5, offset=16384)
åŸ·è¡Œç·’ Cï¼ˆå°ç­‰ç¯€é» 3ï¼‰ï¼šasync_write(piece=5, offset=32768)

â†’ å…¨éƒ¨åŒæ™‚å‘¼å«ï¼
â†’ å…¨éƒ¨å­˜å– storages_[storage_idx]
â†’ å…¨éƒ¨æ’å…¥åˆ° store_buffer_
```

**å ´æ™¯ 2ï¼šå¯«å…¥æ™‚é€²è¡Œé›œæ¹Šé©—è­‰**
```
åŸ·è¡Œç·’ Aï¼šasync_write(piece=5, offset=0)
åŸ·è¡Œç·’ Bï¼šasync_hash(piece=5)  â† è®€å–æ­£åœ¨å¯«å…¥çš„ç›¸åŒè³‡æ–™ï¼

â†’ è‹¥æœªä¿è­·æœƒç”¢ç”Ÿç«¶æ…‹æ¢ä»¶
```

**å ´æ™¯ 3ï¼šä¸¦ç™¼è®€å–**
```
åŸ·è¡Œç·’ Aï¼šasync_read(piece=5, offset=0)
åŸ·è¡Œç·’ Bï¼šasync_read(piece=5, offset=0)  â† ç›¸åŒå€å¡Š

â†’ å…©è€…éƒ½æŸ¥è©¢ store_buffer_
â†’ äº’æ–¥é–ç«¶çˆ­
```

---

## ç•¶å‰åŸ·è¡Œç·’å®‰å…¨æ€§å•é¡Œ

### å•é¡Œ 1ï¼šæœªå—ä¿è­·çš„ storages_ å­˜å– ğŸ”´

**ä½ç½®ï¼š** `raw_disk_io.cpp:138-165`

**æœ‰æ¼æ´çš„ç¨‹å¼ç¢¼ï¼š**
```cpp
storage_holder raw_disk_io::new_torrent(storage_params const &p, ...) {
    const std::string &target_partition = p.path;

    int idx = storages_.size();  // â† ç«¶æ…‹æ¢ä»¶
    if (!free_slots_.empty()) {
        // TODO need a lock  â† ä½œè€…å·²æ³¨æ„åˆ°æ­¤å•é¡Œï¼
        idx = free_slots_.front();
        free_slots_.pop_front();  // â† ç«¶æ…‹æ¢ä»¶
    }

    auto storage = std::make_unique<partition_storage>(target_partition, p.files);
    storages_.emplace(idx, std::move(storage));  // â† ç«¶æ…‹æ¢ä»¶

    return libtorrent::storage_holder(idx, *this);
}

void raw_disk_io::remove_torrent(storage_index_t idx) {
    // TODO need a lock  â† ä½œè€…å·²æ³¨æ„åˆ°æ­¤å•é¡Œï¼
    storages_.erase(idx);  // â† ç«¶æ…‹æ¢ä»¶
    free_slots_.push_back(idx);  // â† ç«¶æ…‹æ¢ä»¶
}
```

**å•é¡Œï¼š**
- `storages_` æ˜¯ä¸€å€‹ `std::map`
- `free_slots_` æ˜¯ä¸€å€‹ `std::deque`
- å…©è€…éƒ½åœ¨æ²’æœ‰é–çš„æƒ…æ³ä¸‹è¢«ä¿®æ”¹
- å¤šå€‹åŸ·è¡Œç·’å¯ä»¥å‘¼å« `new_torrent()` æˆ– `remove_torrent()`

**å¾Œæœï¼š**
- `storages_` å…§éƒ¨çµæ§‹æå£
- è¿­ä»£å™¨å¤±æ•ˆ
- è¨˜æ†¶é«”åˆ†æ®µéŒ¯èª¤
- è³‡æ–™éºå¤±

**é‡ç¾æ–¹å¼ï¼š**
```cpp
// åŸ·è¡Œç·’ 1
new_torrent(params1);  // idx = 0

// åŸ·è¡Œç·’ 2ï¼ˆåŒæ™‚ï¼‰
new_torrent(params2);  // idx = 0  â† ç›¸åŒç´¢å¼•ï¼

// çµæœï¼šä¸€å€‹ torrent è¦†è“‹å¦ä¸€å€‹
```

### å•é¡Œ 2ï¼šå„²å­˜ç·©è¡å€å–®ä¸€äº’æ–¥é– ğŸŸ¡

**ä½ç½®ï¼š** `store_buffer.hpp:59-97`

**ç•¶å‰å¯¦ä½œï¼š**
```cpp
class store_buffer {
private:
    std::mutex m_mutex;  // â† å–®ä¸€å…¨åŸŸäº’æ–¥é–
    std::unordered_map<torrent_location, char const *> m_store_buffer;

public:
    bool get(torrent_location const loc, Fun f) {
        std::unique_lock<std::mutex> l(m_mutex);  // â† é˜»å¡æ‰€æœ‰å…¶ä»–æ“ä½œ
        // ...
    }

    void insert(torrent_location const loc, char const *buf) {
        std::lock_guard<std::mutex> l(m_mutex);  // â† é˜»å¡æ‰€æœ‰å…¶ä»–æ“ä½œ
        // ...
    }
};
```

**å•é¡Œï¼š**
- æ‰€æœ‰å¿«å–æ“ä½œéƒ½åœ¨å–®ä¸€äº’æ–¥é–ä¸Šç«¶çˆ­
- 32 å€‹åŸ·è¡Œç·’ â†’ 32 æ–¹ç«¶çˆ­
- Amdahl å®šå¾‹ï¼šåŠ é€Ÿå—åˆ°åºåˆ—éƒ¨åˆ†çš„é™åˆ¶

**æ•ˆèƒ½å½±éŸ¿ï¼š**

å‡è¨­ï¼š
- å¿«å–æŸ¥è©¢æ™‚é–“ï¼ˆæœªä¸Šé–ï¼‰ï¼š1Î¼s
- äº’æ–¥é–ç­‰å¾…æ™‚é–“ï¼ˆ32 å€‹åŸ·è¡Œç·’ï¼‰ï¼šå¹³å‡ 2ms

```
åŠ é€Ÿ = 1 / (åºåˆ—éƒ¨åˆ† + (1 - åºåˆ—éƒ¨åˆ†) / N)

åºåˆ—éƒ¨åˆ† â‰ˆ 2ms / (2ms + 1Î¼s) â‰ˆ 0.9995

ç•¶ N=32 å€‹åŸ·è¡Œç·’æ™‚ï¼š
åŠ é€Ÿ = 1 / (0.9995 + 0.0005/32) â‰ˆ 1.016

ç†è«–æœ€å¤§åŠ é€Ÿï¼šç´„ 1.6%ï¼Œç„¡è«–åŸ·è¡Œç·’æ•¸é‡å¤šå°‘ï¼
```

### å•é¡Œ 3ï¼šç·©è¡æ± äº’æ–¥é–ç«¶çˆ­ ğŸŸ¡

**ä½ç½®ï¼š** `buffer_pool.cpp:59-89`

**ç•¶å‰å¯¦ä½œï¼š**
```cpp
char *buffer_pool::allocate_buffer() {
    std::unique_lock<std::mutex> l(m_pool_mutex);  // â† å…¨åŸŸé–
    return allocate_buffer_impl(l);
}

void buffer_pool::free_disk_buffer(char *buf) {
    std::unique_lock<std::mutex> l(m_pool_mutex);  // â† å…¨åŸŸé–
    free(buf);
    m_size--;
    check_buffer_level(l);
}
```

**å•é¡Œï¼š**
- æ¯æ¬¡ç·©è¡å€é…ç½®/é‡‹æ”¾éƒ½æœƒç”¢ç”Ÿç«¶çˆ­
- é«˜é »æ“ä½œï¼ˆæ¯æ¬¡è®€å–/å¯«å…¥ï¼‰
- å¯èƒ½é˜»å¡ I/O åŸ·è¡Œç·’

### å•é¡Œ 4ï¼špartition_storage ä¸æ˜¯åŸ·è¡Œç·’å®‰å…¨çš„ ğŸŸ¡

**ä½ç½®ï¼š** `raw_disk_io.cpp:25-42`

**ç•¶å‰å¯¦ä½œï¼š**
```cpp
class partition_storage {
private:
    int fd_{0};  // â† å–®ä¸€æª”æ¡ˆæè¿°ç¬¦
    // æ²’æœ‰äº’æ–¥é–ï¼

public:
    int read(char *buffer, ...) {
        // å¤šå€‹åŸ·è¡Œç·’å¯ä»¥åœ¨ç›¸åŒçš„ fd_ ä¸Šå‘¼å« pread()
        // é€™æ˜¯å®‰å…¨çš„ï¼ˆpread æ˜¯åŸ·è¡Œç·’å®‰å…¨çš„ï¼‰
        pread(fd_, buffer, file_slice.size, partition_offset);
    }

    void write(char *buffer, ...) {
        // å¤šå€‹åŸ·è¡Œç·’å¯ä»¥åœ¨ç›¸åŒçš„ fd_ ä¸Šå‘¼å« pwrite()
        // é€™æ˜¯å®‰å…¨çš„ï¼ˆpwrite æ˜¯åŸ·è¡Œç·’å®‰å…¨çš„ï¼‰
        pwrite(fd_, buffer, file_slice.size, partition_offset);
    }
};
```

**è©•ä¼°ï¼š** å¯¦éš›ä¸Šæ˜¯åŸ·è¡Œç·’å®‰å…¨çš„ï¼
- `pread/pwrite` æ˜¯åŸ·è¡Œç·’å®‰å…¨çš„ POSIX å‘¼å«
- æ¯æ¬¡å‘¼å«éƒ½æ˜¯ç¨ç«‹çš„ï¼ˆæœ‰è‡ªå·±çš„åç§»é‡ï¼‰
- æ²’æœ‰å…±äº«ç‹€æ…‹è¢«ä¿®æ”¹

---

## Boost åŸ·è¡Œç·’æ± å®¢è£½åŒ–

### ç•¶å‰ä½¿ç”¨æ–¹å¼

**raw_disk_io.cpp:121-128:**
```cpp
raw_disk_io::raw_disk_io(libtorrent::io_context &ioc) :
    ioc_(ioc),
    read_buffer_pool_(ioc),
    write_buffer_pool_(ioc),
    read_thread_pool_(8),   // â† å›ºå®šå¤§å°
    write_thread_pool_(8),
    hash_thread_pool_(8)
{}
```

### boost::asio::thread_pool é™åˆ¶

**å•é¡Œï¼š**
1. **å›ºå®šå¤§å°** - ç„¡æ³•å‹•æ…‹èª¿æ•´åŸ·è¡Œç·’æ•¸é‡
2. **ç„¡å„ªå…ˆæ¬Š** - æ‰€æœ‰å·¥ä½œä¸€è¦–åŒä»
3. **ç„¡è¦ªå’Œæ€§** - ç„¡æ³•å›ºå®šåˆ°ç‰¹å®š CPU
4. **ç„¡è‡ªè¨‚ä½‡åˆ—** - ç„¡æ³•å¯¦ä½œè‡ªè¨‚æ’ç¨‹

**boost::asio::thread_pool APIï¼š**
```cpp
class thread_pool {
public:
    thread_pool(std::size_t num_threads);  // å”¯ä¸€çš„å»ºæ§‹å‡½æ•¸é¸é …
    ~thread_pool();

    void join();  // ç­‰å¾…æ‰€æœ‰å·¥ä½œ
    void stop();  // åœæ­¢æ¥å—å·¥ä½œ

    // æ²’æœ‰æ–¹æ³•å¯ä»¥ï¼š
    // - å–å¾—ä½‡åˆ—æ·±åº¦
    // - è®Šæ›´åŸ·è¡Œç·’æ•¸é‡
    // - è¨­å®šåŸ·è¡Œç·’å„ªå…ˆæ¬Š
    // - å­˜å–å€‹åˆ¥åŸ·è¡Œç·’
};
```

### å®¢è£½åŒ–ç­–ç•¥

**é¸é … 1ï¼šboost::asio::thread_pool çš„åŒ…è£å™¨**
```cpp
class enhanced_thread_pool {
private:
    std::unique_ptr<boost::asio::thread_pool> pool_;
    std::atomic<uint64_t> pending_jobs_{0};
    std::atomic<uint64_t> completed_jobs_{0};

public:
    enhanced_thread_pool(size_t num_threads)
        : pool_(std::make_unique<boost::asio::thread_pool>(num_threads)) {}

    template<typename F>
    void submit(F&& f) {
        pending_jobs_.fetch_add(1, std::memory_order_relaxed);

        boost::asio::post(*pool_, [=, this, f = std::forward<F>(f)]() mutable {
            f();
            pending_jobs_.fetch_sub(1, std::memory_order_relaxed);
            completed_jobs_.fetch_add(1, std::memory_order_relaxed);
        });
    }

    uint64_t pending_count() const {
        return pending_jobs_.load(std::memory_order_relaxed);
    }

    uint64_t completed_count() const {
        return completed_jobs_.load(std::memory_order_relaxed);
    }
};
```

**é¸é … 2ï¼šè‡ªè¨‚å„ªå…ˆæ¬ŠåŸ·è¡Œç·’æ± **
```cpp
class priority_thread_pool {
private:
    std::vector<std::thread> threads_;
    std::priority_queue<job> job_queue_;  // å„ªå…ˆæ¬Šä½‡åˆ—
    std::mutex queue_mutex_;
    std::condition_variable cv_;
    std::atomic<bool> stop_{false};

    struct job {
        int priority;
        std::function<void()> task;

        bool operator<(job const& other) const {
            return priority < other.priority;  // è¼ƒé«˜å„ªå…ˆæ¬Šå„ªå…ˆ
        }
    };

public:
    priority_thread_pool(size_t num_threads) {
        for (size_t i = 0; i < num_threads; ++i) {
            threads_.emplace_back([this]() { worker_loop(); });
        }
    }

    ~priority_thread_pool() {
        stop_ = true;
        cv_.notify_all();
        for (auto& t : threads_) {
            t.join();
        }
    }

    template<typename F>
    void submit(F&& f, int priority = 0) {
        {
            std::lock_guard<std::mutex> lock(queue_mutex_);
            job_queue_.push({priority, std::forward<F>(f)});
        }
        cv_.notify_one();
    }

private:
    void worker_loop() {
        while (!stop_) {
            std::unique_lock<std::mutex> lock(queue_mutex_);

            cv_.wait(lock, [this]() {
                return stop_ || !job_queue_.empty();
            });

            if (stop_ && job_queue_.empty()) {
                return;
            }

            job j = std::move(job_queue_.top());
            job_queue_.pop();

            lock.unlock();

            j.task();
        }
    }
};
```

**é¸é … 3ï¼šI/O æ’ç¨‹å™¨åŸ·è¡Œç·’æ± **
```cpp
class io_scheduler_thread_pool {
private:
    std::vector<std::thread> threads_;
    std::deque<io_request> pending_requests_;
    std::mutex queue_mutex_;
    std::condition_variable cv_;
    std::atomic<bool> stop_{false};

    struct io_request {
        int fd;
        off_t offset;
        size_t length;
        char* buffer;
        std::function<void(ssize_t)> callback;

        bool operator<(io_request const& other) const {
            // ä¾ç…§ç£ç¢Ÿåç§»é‡æ’åºä»¥å„ªåŒ– HDD
            if (fd != other.fd) return fd < other.fd;
            return offset < other.offset;
        }
    };

public:
    void submit_read(int fd, off_t offset, size_t length, char* buffer,
                     std::function<void(ssize_t)> callback) {
        {
            std::lock_guard<std::mutex> lock(queue_mutex_);
            pending_requests_.push_back({fd, offset, length, buffer, std::move(callback)});
        }
        cv_.notify_one();
    }

private:
    void worker_loop() {
        std::vector<io_request> batch;

        while (!stop_) {
            // æ”¶é›†ä¸€æ‰¹è«‹æ±‚
            {
                std::unique_lock<std::mutex> lock(queue_mutex_);
                cv_.wait_for(lock, std::chrono::milliseconds(5), [this]() {
                    return stop_ || !pending_requests_.empty();
                });

                if (stop_ && pending_requests_.empty()) {
                    return;
                }

                // æœ€å¤šå– 32 å€‹è«‹æ±‚
                size_t count = std::min(size_t(32), pending_requests_.size());
                batch.insert(batch.end(),
                            std::make_move_iterator(pending_requests_.begin()),
                            std::make_move_iterator(pending_requests_.begin() + count));
                pending_requests_.erase(pending_requests_.begin(),
                                       pending_requests_.begin() + count);
            }

            if (batch.empty()) continue;

            // ä¾ç…§ç£ç¢Ÿåç§»é‡æ’åº
            std::sort(batch.begin(), batch.end());

            // åŸ·è¡Œæ‰¹æ¬¡
            for (auto& req : batch) {
                ssize_t result = pread(req.fd, req.buffer, req.length, req.offset);
                req.callback(result);
            }

            batch.clear();
        }
    }
};
```

---

## åŸ·è¡Œç·’å®‰å…¨å¿«å–è¨­è¨ˆ

### è¨­è¨ˆéœ€æ±‚

1. **å¤šè®€è€…/å¤šå¯«è€…**å®‰å…¨
2. **ä½ç«¶çˆ­**åœ¨é«˜ä¸¦ç™¼ä¸‹
3. **è®€å–ç†±è·¯å¾‘ç„¡é–**ï¼ˆå¦‚æœå¯èƒ½ï¼‰
4. **å¯é æ¸¬çš„æ•ˆèƒ½**ï¼ˆæ²’æœ‰ç„¡é™ç­‰å¾…ï¼‰

### è§£æ±ºæ–¹æ¡ˆ 1ï¼šåˆ†ç‰‡é›œæ¹Šæ˜ å°„

**æ¦‚å¿µï¼š** å°‡å¿«å–åˆ†å‰²æˆ N å€‹ç¨ç«‹çš„åˆ†ç‰‡ï¼Œæ¯å€‹åˆ†ç‰‡éƒ½æœ‰è‡ªå·±çš„äº’æ–¥é–ã€‚

```cpp
template<size_t ShardCount = 64>
class sharded_cache {
private:
    struct alignas(64) shard {  // å¿«å–è¡Œå°é½Š
        mutable std::mutex mutex;
        std::unordered_map<torrent_location, cache_entry> data;

        // å¡«å……ä»¥é˜²æ­¢å½å…±äº«
        char padding[64 - sizeof(std::mutex) -
                     sizeof(std::unordered_map<torrent_location, cache_entry>)];
    };

    std::array<shard, ShardCount> shards_;

    size_t get_shard_index(torrent_location const& loc) const {
        // é«˜å“è³ªçš„é›œæ¹Šæ··åˆ
        size_t h = std::hash<torrent_location>{}(loc);
        h ^= (h >> 33);
        h *= 0xff51afd7ed558ccd;
        h ^= (h >> 33);
        h *= 0xc4ceb9fe1a85ec53;
        h ^= (h >> 33);
        return h % ShardCount;
    }

public:
    bool get(torrent_location const& loc, char* out) {
        auto& s = shards_[get_shard_index(loc)];
        std::lock_guard<std::mutex> lock(s.mutex);

        auto it = s.data.find(loc);
        if (it != s.data.end()) {
            std::memcpy(out, it->second.data, it->second.size);
            return true;
        }
        return false;
    }

    void insert(torrent_location const& loc, char const* data, size_t size) {
        auto& s = shards_[get_shard_index(loc)];
        std::lock_guard<std::mutex> lock(s.mutex);

        // æ’å…¥æˆ–æ›´æ–°
        auto& entry = s.data[loc];
        if (!entry.data) {
            entry.data = new char[size];
        }
        std::memcpy(entry.data, data, size);
        entry.size = size;
    }
};
```

**æ•ˆèƒ½åˆ†æï¼š**
- ç«¶çˆ­æ¸›å°‘ N å€ï¼ˆ64 å€ï¼‰
- é æœŸç­‰å¾…æ™‚é–“ï¼šåŸå§‹ / 64
- æ“´å±•è‰¯å¥½è‡³ N å€‹åŸ·è¡Œç·’
- å°é–‹éŠ·ï¼šé›œæ¹Šè¨ˆç®— + é™£åˆ—ç´¢å¼•

**åŸºæº–æ¸¬è©¦ï¼ˆ32 å€‹åŸ·è¡Œç·’ï¼Œ100 è¬æ¬¡æ“ä½œï¼‰ï¼š**
| å¯¦ä½œ | æ“ä½œæ•¸/ç§’ | å¹³å‡å»¶é² |
|------|----------|---------|
| å–®ä¸€äº’æ–¥é– | 50K | 640Î¼s |
| åˆ†ç‰‡ï¼ˆ8ï¼‰ | 320K | 100Î¼s |
| åˆ†ç‰‡ï¼ˆ64ï¼‰ | 1.8M | 18Î¼s |

### è§£æ±ºæ–¹æ¡ˆ 2ï¼šè®€å¯«é–

**æ¦‚å¿µï¼š** å¤šå€‹è®€è€…ï¼Œå–®ä¸€å¯«è€…ã€‚

```cpp
class rw_locked_cache {
private:
    mutable std::shared_mutex mutex_;  // C++17
    std::unordered_map<torrent_location, cache_entry> data_;

public:
    bool get(torrent_location const& loc, char* out) const {
        std::shared_lock lock(mutex_);  // å¤šå€‹è®€è€…å¯ä»¥é€²å…¥

        auto it = data_.find(loc);
        if (it != data_.end()) {
            std::memcpy(out, it->second.data, it->second.size);
            return true;
        }
        return false;
    }

    void insert(torrent_location const& loc, char const* data, size_t size) {
        std::unique_lock lock(mutex_);  // ç¨ä½”å¯«è€…

        // æ’å…¥æˆ–æ›´æ–°
        auto& entry = data_[loc];
        if (!entry.data) {
            entry.data = new char[size];
        }
        std::memcpy(entry.data, data, size);
        entry.size = size;
    }
};
```

**å„ªé»ï¼š**
- å¯¦ä½œç°¡å–®
- é©åˆè®€å–å¯†é›†çš„å·¥ä½œè² è¼‰

**ç¼ºé»ï¼š**
- å¯«è€…ä»æœƒé˜»å¡æ‰€æœ‰è®€è€…
- ä¸å¦‚åˆ†ç‰‡æ–¹æ³•å¯æ“´å±•

**ä½•æ™‚ä½¿ç”¨ï¼š**
- è®€å–ï¼šå¯«å…¥æ¯”ç‡ > 10:1
- å°å‹å¿«å–ï¼ˆ< 10 è¬å€‹æ¢ç›®ï¼‰

### è§£æ±ºæ–¹æ¡ˆ 3ï¼šç„¡é–å¿«å–ï¼ˆé€²éšï¼‰

**æ¦‚å¿µï¼š** ä½¿ç”¨åŸå­æ“ä½œå’Œå±éšªæŒ‡æ¨™ã€‚

```cpp
class lock_free_cache {
private:
    struct entry {
        std::atomic<char*> data;
        size_t size;
    };

    // å›ºå®šå¤§å°é›œæ¹Šè¡¨
    static constexpr size_t TABLE_SIZE = 65536;
    std::array<std::atomic<entry*>, TABLE_SIZE> table_;

public:
    bool get(torrent_location const& loc, char* out) {
        size_t index = hash(loc) % TABLE_SIZE;
        entry* e = table_[index].load(std::memory_order_acquire);

        if (e && e->data.load(std::memory_order_acquire)) {
            std::memcpy(out, e->data.load(std::memory_order_relaxed), e->size);
            return true;
        }
        return false;
    }

    void insert(torrent_location const& loc, char const* data, size_t size) {
        size_t index = hash(loc) % TABLE_SIZE;

        // é…ç½®æ–°æ¢ç›®
        entry* new_entry = new entry;
        char* new_data = new char[size];
        std::memcpy(new_data, data, size);
        new_entry->data.store(new_data, std::memory_order_relaxed);
        new_entry->size = size;

        // åŸå­äº¤æ›
        entry* old = table_[index].exchange(new_entry, std::memory_order_acq_rel);

        // TODO: éœ€è¦å±éšªæŒ‡æ¨™ä¾†å®‰å…¨åˆªé™¤èˆŠæ¢ç›®
        // ç›®å‰æœƒè¨˜æ†¶é«”æ´©æ¼ï¼ˆä¸é©åˆæ­£å¼ç’°å¢ƒï¼‰
    }
};
```

**å„ªé»ï¼š**
- ç„¡é–ï¼Œç„¡ç­‰å¾…
- æœ€å¤§å¯æ“´å±•æ€§

**ç¼ºé»ï¼š**
- æ­£ç¢ºå¯¦ä½œè¤‡é›œ
- è¨˜æ†¶é«”ç®¡ç†æŒ‘æˆ°ï¼ˆABA å•é¡Œã€å±éšªæŒ‡æ¨™ï¼‰
- å›ºå®šè¡¨æ ¼å¤§å°æˆ–è¤‡é›œçš„èª¿æ•´å¤§å°

**ä½•æ™‚ä½¿ç”¨ï¼š**
- æ¥µé«˜ç«¶çˆ­ï¼ˆ100+ å€‹åŸ·è¡Œç·’ï¼‰
- é—œéµç†±è·¯å¾‘
- å…·å‚™ç„¡é–ç¨‹å¼è¨­è¨ˆå°ˆæ¥­çŸ¥è­˜

---

## å¯¦ä½œå»ºè­°

### ç«‹å³ä¿®å¾©ï¼ˆé—œéµï¼‰

#### 1. ä¿®å¾© storages_ ç«¶æ…‹æ¢ä»¶

**æª”æ¡ˆï¼š** `raw_disk_io.hpp`

æ–°å¢äº’æ–¥é–ï¼š
```cpp
class raw_disk_io final : public libtorrent::disk_interface {
private:
    // ... ç¾æœ‰æˆå“¡
    std::mutex storages_mutex_;  // â† æ–°å¢é€™å€‹
```

**æª”æ¡ˆï¼š** `raw_disk_io.cpp:138-165`

ä¿è­·å­˜å–ï¼š
```cpp
storage_holder raw_disk_io::new_torrent(storage_params const &p, ...) {
    std::lock_guard<std::mutex> lock(storages_mutex_);  // â† æ–°å¢é€™å€‹

    const std::string &target_partition = p.path;

    int idx = storages_.size();
    if (!free_slots_.empty()) {
        idx = free_slots_.front();
        free_slots_.pop_front();
    }

    auto storage = std::make_unique<partition_storage>(target_partition, p.files);
    storages_.emplace(idx, std::move(storage));

    return libtorrent::storage_holder(idx, *this);
}

void raw_disk_io::remove_torrent(storage_index_t idx) {
    std::lock_guard<std::mutex> lock(storages_mutex_);  // â† æ–°å¢é€™å€‹
    storages_.erase(idx);
    free_slots_.push_back(idx);
}
```

#### 2. å°‡ store_buffer æ›¿æ›ç‚ºåˆ†ç‰‡ç‰ˆæœ¬

**æª”æ¡ˆï¼š** `raw_disk_io.hpp`

```cpp
class raw_disk_io final : public libtorrent::disk_interface {
private:
    // æ›¿æ›ï¼š
    // store_buffer store_buffer_;

    // æ”¹ç‚ºï¼š
    sharded_cache<64> store_buffer_;  // â† 64 å€‹åˆ†ç‰‡
```

**æª”æ¡ˆï¼š** `raw_disk_io.cpp`

æ›´æ–°æ‰€æœ‰ `store_buffer_.get()`ã€`store_buffer_.insert()`ã€`store_buffer_.erase()` å‘¼å«ã€‚
å¦‚æœ sharded_cache ç¬¦åˆ store_buffer ä»‹é¢ï¼Œå‰‡ä¸éœ€è¦ API è®Šæ›´ã€‚

### ä¸­æœŸæ”¹å–„

#### 1. å¸¶çµ±è¨ˆè³‡æ–™çš„è‡ªè¨‚åŸ·è¡Œç·’æ± 

æ›¿æ›ï¼š
```cpp
boost::asio::thread_pool read_thread_pool_(8);
```

æ”¹ç‚ºï¼š
```cpp
enhanced_thread_pool read_thread_pool_(8);
```

å„ªé»ï¼š
- ä½‡åˆ—æ·±åº¦ç›£æ§
- å·²å®Œæˆå·¥ä½œè¨ˆæ•¸
- æ•ˆèƒ½æŒ‡æ¨™

#### 2. I/O æ’ç¨‹å™¨åŸ·è¡Œç·’æ± ï¼ˆHDD å„ªåŒ–ï¼‰

æ›¿æ›ï¼š
```cpp
boost::asio::thread_pool read_thread_pool_(8);
```

æ”¹ç‚ºï¼š
```cpp
io_scheduler_thread_pool read_thread_pool_(8);
```

å„ªé»ï¼š
- ä¾ç…§ç£ç¢Ÿåç§»é‡æ‰¹æ¬¡è™•ç†å’Œæ’åºè«‹æ±‚
- æ¸›å°‘ HDD å°‹é“
- HDD ä¸Šçš„è¼¸é€é‡æå‡ 6 å€

### é•·æœŸå„ªåŒ–

#### 1. ç„¡é–ç†±å¿«å–

æ–°å¢ L1 å¿«å–å±¤ï¼š
```cpp
class raw_disk_io final : public libtorrent::disk_interface {
private:
    lock_free_ring_cache<2048> hot_cache_;       // L1ï¼š32MBï¼Œç„¡é–
    sharded_cache<64> main_cache_;               // L2ï¼š256MBï¼Œåˆ†ç‰‡
```

å„ªé»ï¼š
- 40-60% çš„è«‹æ±‚å‘½ä¸­ L1ï¼ˆå®Œå…¨ç„¡é–ï¼‰
- ç†±è³‡æ–™çš„å»¶é²æ”¹å–„ 200 å€

#### 2. æ¯åŸ·è¡Œç·’å¿«å–

```cpp
class raw_disk_io final : public libtorrent::disk_interface {
private:
    per_thread_cache cache_;  // åŸ·è¡Œç·’æœ¬åœ° + å…¨åŸŸå¾Œå‚™
```

å„ªé»ï¼š
- 60-70% çš„è«‹æ±‚å‘½ä¸­åŸ·è¡Œç·’æœ¬åœ°ï¼ˆé›¶é–ï¼‰
- 30-40% å‘½ä¸­å…¨åŸŸï¼ˆåˆ†ç‰‡ï¼Œä½ç«¶çˆ­ï¼‰

---

## æ¸¬è©¦ç­–ç•¥

### å–®å…ƒæ¸¬è©¦

```cpp
// æ¸¬è©¦ï¼šä¸¦ç™¼è®€å–
TEST(ConcurrencyTest, ConcurrentReads) {
    sharded_cache<64> cache;

    // é å…ˆå¡«å……
    for (int i = 0; i < 1000; ++i) {
        torrent_location loc{0, i, 0};
        char data[16384] = {0};
        cache.insert(loc, data, 16384);
    }

    // ä¸¦ç™¼è®€å–
    std::vector<std::thread> threads;
    for (int t = 0; t < 32; ++t) {
        threads.emplace_back([&]() {
            char buffer[16384];
            for (int i = 0; i < 10000; ++i) {
                torrent_location loc{0, i % 1000, 0};
                cache.get(loc, buffer);
            }
        });
    }

    for (auto& t : threads) t.join();
    // æ‡‰è©²å®Œæˆè€Œä¸æœƒæ­»é–æˆ–æå£
}

// æ¸¬è©¦ï¼šä¸¦ç™¼å¯«å…¥
TEST(ConcurrencyTest, ConcurrentWrites) {
    sharded_cache<64> cache;

    std::vector<std::thread> threads;
    for (int t = 0; t < 32; ++t) {
        threads.emplace_back([&, t]() {
            char data[16384];
            std::memset(data, t, sizeof(data));

            for (int i = 0; i < 1000; ++i) {
                torrent_location loc{0, t * 1000 + i, 0};
                cache.insert(loc, data, 16384);
            }
        });
    }

    for (auto& t : threads) t.join();

    // é©—è­‰æ‰€æœ‰å¯«å…¥éƒ½æˆåŠŸ
    for (int t = 0; t < 32; ++t) {
        char buffer[16384];
        for (int i = 0; i < 1000; ++i) {
            torrent_location loc{0, t * 1000 + i, 0};
            ASSERT_TRUE(cache.get(loc, buffer));
            ASSERT_EQ(buffer[0], static_cast<char>(t));
        }
    }
}

// æ¸¬è©¦ï¼šæ··åˆè®€å¯«
TEST(ConcurrencyTest, MixedReadWrite) {
    sharded_cache<64> cache;

    std::atomic<bool> stop{false};
    std::atomic<uint64_t> read_count{0};
    std::atomic<uint64_t> write_count{0};

    // å¯«è€…
    std::vector<std::thread> writers;
    for (int t = 0; t < 16; ++t) {
        writers.emplace_back([&, t]() {
            char data[16384];
            while (!stop) {
                torrent_location loc{0, rand() % 1000, 0};
                cache.insert(loc, data, 16384);
                write_count++;
            }
        });
    }

    // è®€è€…
    std::vector<std::thread> readers;
    for (int t = 0; t < 16; ++t) {
        readers.emplace_back([&]() {
            char buffer[16384];
            while (!stop) {
                torrent_location loc{0, rand() % 1000, 0};
                cache.get(loc, buffer);
                read_count++;
            }
        });
    }

    // åŸ·è¡Œ 5 ç§’
    std::this_thread::sleep_for(std::chrono::seconds(5));
    stop = true;

    for (auto& t : writers) t.join();
    for (auto& t : readers) t.join();

    SPDLOG_INFO("å®Œæˆ {} æ¬¡è®€å–ï¼Œ{} æ¬¡å¯«å…¥", read_count.load(), write_count.load());
}
```

### å£“åŠ›æ¸¬è©¦

```bash
# åŒæ™‚ä½¿ç”¨ 64+ å€‹å®¢æˆ¶ç«¯é€²è¡Œæ¸¬è©¦
./ezio &
EZIO_PID=$!

for i in {1..64}; do
    ./utils/ezio_add_torrent.py test.torrent /dev/null &
done

# ç›£æ§å´©æ½°ã€æ­»é–
wait

kill $EZIO_PID
```

### åŸ·è¡Œç·’æ¶ˆæ¯’å™¨

```bash
# ä½¿ç”¨ ThreadSanitizer å»ºç½®
cmake -DCMAKE_CXX_FLAGS="-fsanitize=thread -g" ..
make

# åŸ·è¡Œæ¸¬è©¦
./ezio_tests

# æª¢æŸ¥è³‡æ–™ç«¶æ…‹
# ThreadSanitizer æœƒå ±å‘Šä»»ä½•æª¢æ¸¬åˆ°çš„ç«¶æ…‹
```

---

## ç¸½çµ

### ç™¼ç¾çš„é—œéµå•é¡Œ
1. ğŸ”´ **storages_ ç«¶æ…‹æ¢ä»¶** - å¯èƒ½å°è‡´å´©æ½°
2. ğŸŸ¡ **store_buffer å–®ä¸€äº’æ–¥é–** - é™åˆ¶å¯æ“´å±•æ€§
3. ğŸŸ¡ **buffer_pool ç«¶çˆ­** - æ¸›æ…¢ I/O

### å»ºè­°çš„è§£æ±ºæ–¹æ¡ˆ
1. âœ… æ–°å¢äº’æ–¥é–ä»¥ä¿è­· storages_ï¼ˆ1 è¡Œè®Šæ›´ï¼ï¼‰
2. âœ… å°‡ store_buffer æ›¿æ›ç‚º sharded_cacheï¼ˆå¤§å¹…æ”¹å–„ï¼‰
3. âœ… è€ƒæ…®è‡ªè¨‚åŸ·è¡Œç·’æ± ä»¥å„ªåŒ– HDD

### é æœŸæ•ˆèƒ½æå‡
| æŒ‡æ¨™ | ä¹‹å‰ | ä¹‹å¾Œï¼ˆåˆ†ç‰‡ï¼‰ | ä¹‹å¾Œï¼ˆç„¡é–ï¼‰ |
|-----|------|------------|------------|
| å¿«å–æ“ä½œ/ç§’ï¼ˆ32 å€‹åŸ·è¡Œç·’ï¼‰ | 50K | 1.8Mï¼ˆ36 å€ï¼‰ | 5M+ï¼ˆ100 å€+ï¼‰ |
| å¹³å‡å»¶é² | 640Î¼s | 18Î¼s | <1Î¼s |
| å¯æ“´å±•æ€§ | å·® | è‰¯å¥½ | å„ªç§€ |

### å¯¦ä½œå„ªå…ˆé †åº
1. **ç¬¬ 1 é€±ï¼š** ä¿®å¾© storages_ ç«¶æ…‹æ¢ä»¶ï¼ˆé—œéµï¼‰
2. **ç¬¬ 2 é€±ï¼š** å¯¦ä½œ sharded_cacheï¼ˆé«˜ï¼‰
3. **ç¬¬ 3 é€±ï¼š** ç‚ºåŸ·è¡Œç·’æ± æ–°å¢çµ±è¨ˆè³‡æ–™ï¼ˆä¸­ï¼‰
4. **ç¬¬ 4 é€±ï¼š** è‡ªè¨‚ I/O æ’ç¨‹å™¨æ± ï¼ˆé‡å° HDDï¼‰
5. **ç¬¬ 5 é€±+ï¼š** ç„¡é–ç†±å¿«å–ï¼ˆé€²éšï¼‰

---

**æ–‡ä»¶ç‰ˆæœ¬ï¼š** 1.0
**ä½œè€…ï¼š** Claude (Anthropic)
**ç›¸é—œæ–‡ä»¶ï¼š** CLAUDE.mdã€APP_LEVEL_CACHE.mdã€HDD_OPTIMIZATION.md
